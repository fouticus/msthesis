\chapter{Results}
\label{chap:results}





\section{Results}

This section first summarizes the effect of protein size on network training time.
Then is expores the behavior of sum and product coupled graph convolutions.
Finally, it compares to PAIRpred and the other variants of graph convolution.


\subsection{Training Time}

Figure \ref{fig:train_times} shows a log-log plot of training time as a function of the number of residue pairs in a given complex.
The lines are roughly linear in this space, indicating a power-law relationship.
Fitting lines to the data shows that the power is slightly over 1.0 in all cases, showing that training time increases roughly linearly in the number of pairs as expected.
This is due solely to the pairwise nature of the problem, and in non-pairwise cases, the training time would likely scale linearly in the number of vertices.


\begin{figure}
	\includegraphics[width=1.0\textwidth]{training_time_product_coupling_1-4.png}
	\caption{Training time as a function of number of residue pairs, for a single no convolution layer, as well as four depths of product coupling, receptive field size 21. Linearity in this log-log plot indicates a power law relationship. In this case, the relationship is near linear, with powers of 1.02, 1.22, 1.25, 1.25, and 1.24 respectively for no convolution 1 layer, and product coupling for 1, 2, 3, and 4 layers. All methods examined had similar power law relationships. 
		\label{fig:train_times}}
\end{figure}




\subsection{Sum and Product Coupling Performance}

\begin{table}
	\begin{center}
		\begin{tabular}{lccccc}
			\toprule
			\multirow{2}{*}{Method} &
			Receptive Field & \multicolumn{4}{c}{Layers Before Merge} \\
			& Size & 1 & {2} & {3} & {4} \\
			\midrule
			No Convolution & N/A & \textbf{0.815} & 0.812 & 0.800 & 0.811  \\\cline{1-6}
			\multirow{2}{*}{Sum Coupled} & 11 & 0.868 & 0.889 & 0.882 & 0.884 \\
			& 21 & 0.875 & \textbf{0.903} & 0.880 & 0.890 \\\cline{1-6}
			\multirow{2}{*}{Product Coupled} & 11 & 0.856 & 0.869 & 0.885 & 0.868 \\
			& 21 & 0.863 & 0.876 & 0.896 & \textbf{0.899} \\
			\bottomrule
		\end{tabular}
		\caption{Median area under the receiver operating characteristic curve (AUC) across all complexes in the test set for two variants of graph convolution, \textit{sum-coupled} and \textit{product-coupled}. Results are shown for two different sizes of receptive field, 11 and 21, for different numbers of convolutional layers before the pairwise merge operation. Bold faced values indicate best performance for each method.}
		\label{tab:med_auc}
	\end{center}
\end{table}

Table \ref{tab:med_auc} shows the results of experiments involving sum coupled and product coupled graph convolution, as well as no graph convolution.
Comparing convolution to no convolution reveals that convolution is beneficial.
Specifically, incorporating information from neighboring residues helps indicate whether a particular residue is part of an interface, which is consistent with the biological properties of interfaces.
It's also clear that a receptive field of size 21 is generally better than 11.
For larger receptive fields, this trend does not continue, as performance drops (data not shown).
Interestingly, this value is approximately the size of a typical interface \hl{get number from basir?}.
When using convolution, performance improves with network depth up to a point, then decreases (this is true for product coupling as well, though the data are not shown).
In contrast, networks without convolution are best with only one pre-merge layer.
This suggests that depth alone does not improve performance, but when convolution is performed, a useful hierarchical representation is learned.
Other applications of deep learning have seen this same trend of increasing and decreasing performance.
Possible explanations include insufficient training data, a vanishing gradient, and a problem which is difficult to optimize~\cite{he2015}.
This results also indicate that in most cases, sum coupled graph convolution outperforms product coupled convolution.
This observation is counterintuitive under the premise that product coupling better exploits structure in the input by associating neighbor and edge through elementwise product.
However, AUC is just one measure of performance, and so RFPP must also be examined.

\begin{table}
	\begin{center}
		\begin{tabular}{lccccc}
			\toprule
			\multirow{2}{*}{Method} &
			Receptive Field & \multicolumn{4}{c}{Layers Before Merge} \\
			& Size & 1 & {2} & {3} & {4} \\
			\midrule
			No Convolution & N/A & \textbf{48} & 55 & 53 & 66 \\\cline{1-6}
			\multirow{2}{*}{Sum Coupled} & 11 & 32 & 28 & 70 & 86 \\
			& 21 & \textbf{26} & 37 & 56 & 63 \\\cline{1-6}
			\multirow{2}{*}{Product Coupled} & 11 & 30 & 46 & 26 & 51 \\
			& 21 & 26 & \textbf{25} & 36 & 37 \\
			\bottomrule
		\end{tabular}
		\caption{Median rank of the first positive prediction (RFPP) across all complexes in the test set for two variants of Graph Convolutional Networks (GCN), \textit{sum-coupled} (SC) and \textit{product-coupled} (PC). Results are shown for two different sizes of receptive field, 11 and 21, for different numbers of convolutional layers before the pairwise merge operation. Bold faced values indicate best performance for each method (lower is better). For a given receptive field size and number of layers, product coupling performs better than sum coupling except for one case.}
		\label{tab:med_rfpp}
	\end{center}
\end{table}

Table \ref{tab:med_rfpp} parallels Table \ref{tab:med_auc} but shows RFPPs instead of AUCs.
As before, convolution generally outperforms no convolution, the few exceptions being sum coupled variants with a smaller receptive field.
In this case, however, product coupling outperforms sum coupling for all but one case.
If we accept that product coupling \emph{does} detect more specific patterns than sum coupling, then a lower RFPP may indicate that this property allows the network to better detect the specific patterns which occur in the most  obvious interface pairs.
Unfortunately, the best RFPP performance does not coincide with the best AUC performance, except when not convolving.
This is unsurprising, since the cross-entropy loss function leads to optimization of performance on \emph{all} pairs, not just the top scoring ones.
In other words, RFPP is not being explicitly optimized in this example, whereas AUC is more closely related to the quantity being optimized.

\begin{figure}
	\includegraphics[width=\textwidth]{med_auc.png}
	\caption{Median area under the receiver operating characteristic curve (AUC) across all complexes in the test set, separated by complex class. Sum and product coupling are shown for two receptive field sizes each (11 and 21), as well as no convolution, for 1-4 pre-merge layers. Product coupling performs better for difficult complexes, but worse overall because ther are far more rigid and medium difficulty complexes.
		\label{fig:med_auc}}
\end{figure}

To understand the behavior of each method in more detail, we can separate performance by the difficulty class of the test proteins.
Each chart in Figure \ref{fig:med_auc} shows performance for a single difficulty class, including rigid, medium difficulty, and difficult, with 33, 16, and 6 complexes respectively.
Here it appears that sum and product coupling are closely matched for rigid and medium difficulty, with sum coupling slightly outperforming product coupling in most cases.
A much more significant difference is seen for the difficult complexes, where product coupling is clearly doing better, particularly for 2 and 3 layers.
This implies that the better performance of sum coupling overall is driven by the distribution of difficulty of complexes in the test set.
In the presence of more difficult complexes, it is likely that product coupling would emerge as the clear winner, restoring our original intuition.

\begin{figure}
	\includegraphics[width=0.8\textwidth]{sum_20_2_histo1.png}
	\caption{Histogram of area under the receiver operating characteristic curve (AUC) for complexes in the test set, colored by difficulty class. Scores are from sum coupled graph convolution with two layers and receptive field size 21, which had the highest median AUC of all methods.}
	\label{fig:histo1}
\end{figure}

For another picture of performance across difficulty classes, we can look at histogram of AUCs, as shown in Figure \ref{fig:histo1}.
These AUCs are heavily skewed left, justifying the choice of median for a summary measure.
Surprisingly, there is no clear divide between rigid, medium difficulty, and difficult classes.
In fact, the worst AUC is a rigid complex, and at least one difficult complex achieves AUC above 0.9.
It appears that the distinguishing characteristic between classes is the number of complexes that achieve above 0.95 AUC.
These "trivial" complexes are most frequent in the rigid class, less so in the medium difficulty class, and absent for the difficult class.
%TODO: so what is it about complexes that makes them easy or hard? look at interface size?	

\begin{figure}
	\includegraphics[width=\textwidth]{med_rfpp.png}
	\caption{Median rank of the first positive prediction (RFPP) across all complexes in the test set, separated by complex class. Vertical axes are log scaled. Sum and product coupling are shown for two receptive field sizes each (11 and 21), as well as no convolution, for 1-4 pre-merge layers. Lower RFPP is better. Best performance on rigid complexes is achieved with just one layer for all networks. As difficulty increases, so does the number of layers needed to achieve best results.
		\label{fig:med_rfpp}}
\end{figure}

RFPP can also be separated by difficulty class.
From figure \ref{fig:med_rfpp} we can observe some heterogeneus behavior across methods, but there are some trends worth noting.
For each class, there appears to be a favored number of layers where a notable dip (improvement) in RFPP is observed. 
This optimal depth appears to increase with complex difficulty, suggesting that harder complexes require more layers to achieve best performance.
This suggests that an ensemble approach with varying depths could perform well on complexes of any difficulty. 
In most cases, product coupling performs the best, except for rigid bodies and difficult complexes with few layers.



\subsection{Comparison to Other Methods}

\begin{table}
	\begin{center}
		\begin{tabular}{l c c c c c }
			\toprule
			
			\multirow{2}{*}{Method} & \multirow{2}{*}{Settings} &  \multicolumn{4}{c}{Layers} \\
											 & & 1 & 2 & 3 & 4 \\
			\midrule
			PAIRpred~\cite{minhas2014}   & N/A  & (\textbf{0.863})^{$\dagger$} & - & - & - \\

			\midrule
			\multirow{2}{*}{PATCHY-SAN~\cite{niepert2016}}  & $\text{RF}=10$ & 0.862 & 0.867 & 0.883 & 0.891 \\
										 & $\text{RF}=20$ & 0.850 & 0.875 & \textbf{0.897} & 0.886 \\
			\midrule
			\multirow{2}{*}{Fingerprint~\cite{duvenaud2015}}  & $\text{RF}=10$ & 0.857 & 0.850 & 0.863 & 0.833 \\
										  & $\text{RF}=20$ & 0.861 & 0.867 & 0.881 & \textbf{0.891} \\
			\midrule

			\multirow{6}{*}{R-GCN~\cite{schlichtkrull2017}} & No basis fns, $\text{RF}=10$ & 0.862 & 0.871 & 0.886 & 0.893 \\
								   & No basis fns, $\text{RF}=20$ & 0.876 & \textbf{0.901} & 0.892 & 0.897 \\
								   & 2 basis fns, $\text{RF}=10$ & 0.851 & 0.872 & 0.779 & -	   \\
								   & 2 basis fns, $\text{RF}=20$ & 0.873 & 0.804 & 0.539 & -     \\
								   & 5 basis fns, $\text{RF}=10$ & 0.870 & 0.747 & 0.748 & -	   \\
								   & 5 basis fns, $\text{RF}=20$ & 0.867 & 0.900 & 0.709 & -     \\
			\midrule
			\multirow{2}{*}{Deep Tensor Networks~\cite{schutt2017}}& $\text{RF}=10$ & 0.853 & 0.872 & 0.878 & 0.861 \\
			& $\text{RF}=20$ & 0.862 & 0.880 & 0.873 & \textbf{0.885} \\
			\midrule
			\multirow{4}{*}{DCNN~\cite{atwood2016}} & 2 hops, $\sigma=2$\AA{} & 0.782 & - & - & - \\
										& 2 hops, $\sigma=4$\AA{} & 0.801 & - & - & - \\
										& 5 hops, $\sigma=2$\AA{} & \textbf{0.838} & - & - & - \\ 
										& 5 hops, $\sigma=4$\AA{} & 0.819 & - & - & - \\ 
												  
			\bottomrule
			
		\end{tabular}
		\caption{Comparison with existing classification methods. $\dagger$ PAIRpred is an SVM-based approach, so the result is not really associated with a layer number}
		\label{tab:results_compare}
	\end{center}
	%\end{minipage}
\end{table}

Figure \ref{tab:results_compare} compares the median AUC of PAIRpred, diffusion convolution (DCNN) with 2 and 5 hops, as well as for distance Gaussian standard deviations of 2 and 4 \AA{}, and the best sum coupled results.
The proposed graph convolutions outperform both existing methods, \hl{in every class (check for PAIRpred when I get numbers, also write more about PAIRpred)}.
For DCNN, 5 hops performs better than 2 hops, presumably because it allows information to propagate further across the graph.
The Gaussian standard deviation directly determines the weights on the edges between vertices, therefore the strength of diffusion along that edge.
In this context, smaller values limit diffusion to a localized neighborhood for each hop, whereas larger values allow diffusion across longer distances.
In the 2 hop DCNN, The larger standard deviation allows more diffusion to occur across the graph, compensating for the limited number of hops.
This explains the overall better performance for $\sigma=4\AA{}$, although for medium and difficult complexes, $\sigma=2\AA{}$ appears to do better. 
In these cases, the degree of conformational change is greater, making information at larger distances less reliable for prediction for a residue.
In contrast, the 5 hop DCNN performed better for $\sigma=2AA{}$, suggesting that the larger number of hops allows sufficient information propagation, eliminating the need for diffusion across greater distances for each hop.
This trend is broken for the medium complexes, however.

These results indicate that sum coupled and product coupled convolutions are learning useful representations of each residue that are useful in performing interface prediction.
Their performance is superior to a state-of-the-art interface prediction method and an existing graph convolution approach. 



\subsection{Filter Visualization}

\hl{develop better intuition of behavior of models.}