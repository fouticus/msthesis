\chapter{Introduction}
\label{chap:intro}

We vs. I?

Many cellular processes rely on proteins, which facilitate these processes via their interactions with one another and other elements within the cell. 
(mention protein interaction networks?)
A more complete understanding of how proteins interact with one another can provide insight into certain diseases, aide pharmaceutical research, and improve our understanding of complex cellular processes. 
Proteins interact with one another (typically in pairs) via an \textit{interface}, which is comprised of the components in each protein which participate in the interaction.
Experimentally identifying the interface between two proteins is a time consuming and expensive process which involves crystallization of the protein complex, imaging via x-ray crystallography or nuclear magnetic resonance, and sequence structure alignment using the electron densities.
Biologists could benefit from \textit{in silico} methods which predict the 3D structure given information about each constituent protein, because it would help to suggest unforeseen interfaces which could help guide wet lab experiments.
Such methods exist, but are often based on energy minimization techniques which are computationally expensive and sometimes suboptimal (check/cite?).
Some methods allow the inclusion of additional information about the protein interface in order to help bias the algorithm towards a correct solution. (cite?)
Such information can be in the form of amino acid pairs, one from each protein, which are believed to constitute part of the interface.

The work described in this paper describes a novel method of predicting which amino acid pairs are believed to constitute part of the protein interface, which is inspired by the success of convolutional neural networks in image processing. 
We represent proteins as graphs and define a convolution operation which receptive field is a subgraph of the protein graph.
Our neural networks convolve over each protein and make predictions on pairs of amino acids of the likelihood that they constitute a part of the interface. 

\section{Proteins}

If DNA is the blueprint of life, then proteins are the realization of those blueprints.
In other words, the information contained in DNA is ultimately used to synthesize proteins.

Proteins are composed of amino acids linked in a chain and held together by covalent bonds.
Amino acids are organic compounds consisting of a central "$\alpha$-carbon" atom, which bonds to an amine group, a carboxyl group, a single hydrogen atom, and a so-called side chain, in a tetrahedral configuration (figure?).
There are 20 different types of amino acid, each of which has a different side chain structure which gives rise to distinct structural and electro-chemical properties. (table?)

Amino acids link together when the nitrogen atom from one protein's amine group bonds covalently with the carbon atom of another protein's carboxyl group, releasing a water molecule in the process.
This covalent bond is called a peptide bond, and an amino acid involved in at least one such bond is referred to as an amino acid \textit{residue}.

A \textit{peptide}, or \textit{peptide chain}, is a linear chain of amino acids held together by peptide bonds, and the \textit{backbone} of the peptide consists of all atoms participating in peptide bonds together with the $\alpha$-carbons.
If a peptide contains several residues it is referred to as a \textit{polypeptide}.
Proteins consist of one or more polypeptide chains which are connected together.
All peptides have a natural ordering of their amino acid residues defined by the order in which they were incorporated into the peptide during protein synthesis. (is this true?)
The \textit{N-terminus} is the first amino acid residue to be synthesized (true?) and has a free amine group, whereas the \textit{C-terminus} is the last to be synthesized (true?) and has a free carboxyl group. 


The side chain can influence how an amino acid interacts with other amino acids or other types of molecules. For example, oppositely charged side chains will be attracted to each other, and polar and non-polar side chains will not strongly interact with each other. 
Such interactions occur between residue side chains in a protein, which influence both the folding of the protein into a 3d structure and the proteinâ€™s proclivity to interact with other proteins.

(describe domains or motifs?)
(mention globular proteins vs other kinds?)


\subsection{Protein Structure}

There are multiple ways to describe protein structure, each operating at a different level of abstraction. 

\textit{Primary structure} refers to the sequence of amino acid residues (from N-terminus to C-terminus) in a single polypeptide chain, and is determined by the sequence of codons in the corresponding coding mRNA from which the protein is translated. 
Each of the 20 amino acid types has a single letter designation for short-hand reference, which is particularly useful when enumerating a protein's sequence.

The physical chemistry associated with peptide bonds gives rise to the property that the Nitrogen and Carbon atoms involved in the peptide bond, along with the adjacent $\alpha$-carbons, all lie within a plane, denoted the \textit{amide plane}.
Each $\alpha$-carbon lie on the intersection between two amide planes, and the planes are free to rotate with respect to each other. 
Their relative orientation is described by two angles, $\psi$ and $\phi$, which describe the rotation of each plane with respect to the $\alpha$-carbon's tetrahedral configuration. (figure?)
In some cases, side chains prohibit certain angle values due to \textit{steric constraints}, which enforce that no two atoms may occupy the same volume of space at the same time.
The angles $\phi$ and $\psi$ allow a certain amount of flexibility in a peptide backbone, which gives rise to higher order structures.

\textit{Secondary structure} describes the common local structures that arise from the interaction of non-adjacent residues.
There are three such categories of local structures: helices, sheets, and "other".
A helix occurs when the polypeptide chain coils into a barrel like structure (like the threads of a screw) and residues from adjacent coils form hydrogen bonds with one another.
There are multiple types of helices, each determined by the sequence-distance between to hydrogen bonded residues, but by far the most common type is the $\alpha$-helix, corresponding to a sequence distance of (???). (figure?)
A sheet occurs when a two non-adjacent sections of the polypeptide align next to each other such that residues in one of the sections form hydrogen bonds with residues in the other section.
Sheets may be parallel, in which the "upstream" portion of each section bond with each other as well as the downstream" portions, or anti-parallel, in which the upstream portion of one section is bonds with the downstream portion of the other section, and vice versa. (figure?)
In either case, these are referred to as $\beta$-sheets. 
The third category of local structure is reserved for those sections of of the polypeptide which are neither part of a helix nor part of a sheet.
These sections are more flexible than helices or sheets due the lack of hydrogen bonds, and therefore are useful in connecting the end of one helix/sheet to the beginning of another. 

Helices and sheets provide some rigidity to the polypeptide chain, but it typically further "folds" together into a globular structure. This resulting 3D structure is considered the \textit{tertiary structure} of a protein. 

Finally, it is common for multiple polypeptide chains combine together into a complex.
Because a polypeptide complex may perform a singular function that is not achievable by each of its constituent chains, it is sometimes referred to as a single protein. 
In this case, the \textit{quaternary structure} describes the manner in which the individual polypeptide chains combine together to form the complex. 



\subsection{Protein Interface Prediction}

The locus of protein interaction is the interface, which is comprised of pairs of residues, one from each interacting protein, which interact with one another and bind the two proteins together.
Such interaction between residues requires electrochemical and geometric compatibility in the local neighborhood of a residue pair.
For example, like-charged residues will be counter-conducive to the formation of an interface due to mutual repulsion.

Historically, there have been two variants of the protein interface prediction problem: \textit{partner independent} and \textit{partner specific}.
The former variant considers a single residue from a single protein and attempts to answer the question: will this residue form a part of an interface with any protein? (check this)
The latter variant considers pairs of residues, each from a different protein, and attempts to answer a more specific question: does this \textit{pair} of residues constitute part of an interface between these two specific proteins. 
The pairwise nature of partner specific prediction poses additional challenges when developing methods to answer the research question, but considering a residue pair instead of a single residue accounts for the potential compatibility of the residues to each other.
It is also arguably more relevant to a biologist, because it specifically identifies a piece of the interface in the form of a residue pair, instead of just given single residue recommendations from each interacting protein. 

Interfaces exhibit a local spatial correlation, where a residue is more likely to be a part of an interface if other nearby residues are also a part of an interface. 
In some cases, proteins undergo conformational change when binding because the presence of another protein changes the most energetically favorable state. 
In fact, sometimes this conformational change can actually facilitate the function of a protein or complex.
Local spatial correlation can aide prediction if information from nearby residues is taken into account during prediction.
On the other hand, conformational change can hinder prediction since the bound conformation is not known a priori and so the prediction must be made on the basis of the unbound protein formations. 

Methods of interface prediction include \textit{docking methods}(cite), \textit{template-based methods}(cite), and \textit{machine learning methods}.

Docking methods predict the 3d bound formation of two proteins in a complex and extract the interface from the predicted formation. 
These methods typically involve modeling the protein system electrical potential(check?) caused by each atom in the proteins, and performing an iterative optimization algorithm to identify a configuration which minimizes the energy of the system. (more detail here and fact check)
Unlike template and machine learning methods, docking methods do not require a library of known interfaces in order to make predictions.

Template based methods make predictions based on similarity to a known template protein complex. 
The protein of interest is compared to a library of proteins whose interface is known, and the interface for the protein of interest is predicted using the interfaces of similar proteins (more detail here and fact check).

Machine learning methods attempt to directly predict the interface rather than comparing against a template complex or predicting the bound formation, however they still use information from a library of complexes whose interfaces are known.
Machine learning approaches have included use of a neural network (cite) and a support vector machine(cite). (more detail here and fact check)
The SVM based approach is called PArtner-specific Interacting Residue PREDictor (PAIRpred) (cite), and utilizes multiple radial basis function kernels. 
PAIRpred incorporates both sequence and structural information of residues, and uses pairwise kernels which operate on pairwise residues.
The method performs well compared to existing docking and machine learning methods (how about template based methods?).

(Talk about neural network approach)


The work described in this paper performs partner specific interface prediction using a more sophisticated neural network architecture than previous methods. 


\section{Artificial Neural Networks}

\textit{Feed forward artificial neural networks}, or less formally, \textit{neural networks}, are a class of machine learning algorithm which are loosely inspired by neuroscientific models of how biological neural networks operate. 
Conceptually, a (artificial) neural network represents a parameterized function that is applied to an input data vector which produces an output, which may be a vector or scalar. 
For example, the input vector may be pixel values in an image of interest, and the output function may be a label applied to that image that indicates its content.

(equation)

This function can be decomposed into a sequence of \textit{layers}, each of which performs a relatively simple mathematical operation on its input to produce a an output.
The first layer is taken to be the input data, and the output of the last layer is taken to be the output of the network. 
All intermediate layers are termed \textit{hidden layers}, each of which accepting the output of the prior layer, performing a mathematical operation, and passing the output to the subsequent layer.
The number of layers in a neural network is taken to be its \textit{depth}.
The operations at each layer commonly take the form of a matrix multiplication, where a matrix of weights is multiplied by the input vector to produce an output vector.
This is called a \textit{dense layer}, and mathematically takes the form:

(equation). (include period)

The number of outputs of a given layer is also denoted the number of \textit{units} (or "neurons") in that layer.
This interpretation indicates that each unit in a layer takes a weighted sum of the inputs (or outputs from the previous layer), with each unit having a different set of weights for each input. 
It is also common to include a bias term in each layer and to apply some sort of nonlinear function elementwise on the weighted sum, and the result is considered the \textit{signal} of the layer.
Despite being conceptually simple, it has been proven that this formulation of artificial neural networks can approximate any function, provided it has enough units (look this up, check details and cite).

The challenge of using neural networks for function approximation is in finding the appropriate set of parameters to approximate the desired function. 
For feed forward neural networks, this is accomplished through a supervised training algorithm called backpropagation. 

(write about backpropagation)
(write about overfitting, etc?)

In recent years (cite), more advanced methods for neural networks have demonstrated success in sophisticated tasks, particularly for image related tasks (cite). 
These advances were catalyzed by the availability of large volumes of labeled image data and the improvements in computing power from, for example, general purpose graphical processing units (GP-GPUs) and distributed systems.
These factors allowed experimentation with larger networks and more complicated layer operations, to include convolutional layers.

(describe dense layers?)

\subsection{Convolutional Neural Networks}

Traditionally, neural networks do not explicitly account for inherent structure in the inputs which may be useful when approximating the output function.
For example, pixels in an image are often spatially correlated, but spatial information is not explicitly given to the network.
Convolution layers are one way to incorporate spatial information in a neural network. 
The name comes from an interpretation of how they operate on a set of inputs with a well defined grid structure, such as a pixelated image or discrete time series, where a filter of weights (e.g. 3x3 pixels for images) is convolved over the input grid.
That is, at each position, the filter weights are multiplied with the corresponding inputs, called the \textit{receptive field}, and summed to produce a scalar value.
As with simpler layers, the output is often passed through a nonlinear function
Because each filter multiplication is associated with a position of the input grid, the output retains a grid structure. 
In color images, each pixel has multiple values associated with it which capture color information, often referred to as \textit{channels}, and the filter typically has the same number of channels as the input. 
A convolutional layer may contain multiple filters, each of which produces a grid of scalar values which can be considered a channel in the output.
The weights in a filter determine the which inputs produce a larger signal and which do not, so a filter can be viewed as detecting certain patterns in the input.
An alternate interpretation of convolutional layers is that weights are being shared across a layer such that each unit in the convolutional layer receives inputs form a different localized region of the input, and that all units share weights. 

(equation)
(figure of convolution?)


Convolutional layers exhibit certain properties which aide in the classification tasks.
\textit{local pattern detection}: convolutional filters are usually significantly smaller than the size of the input, so for any given position of the filter they are operating on a local neighborhood of the input. 
\textit{global application}: since filters are convolved over the input, local patterns can be detected no matter where they occur in the input.
\textit{stackability}: the output of a neural network retains the same grid structure as the input, so convolutional layers can be stacked together.
The receptive field of a unit in a stacked layer consists of units in the prior convolutional layer, each of which has its own receptive field in the input.
Hence stacked layers have effectively larger receptive fields on the original input, and are therefore able to detect larger patterns in the original input which are typically more sophisticated.
Stacked convolutions on images have demonstrated an ability to learn a conceptual hierarchy of patterns in the input, from simple textures and edges to complex objects like faces or vehicles. 
\textit{robustness to overfitting}: the weight sharing inherent in convolutional layers means there are fewer trainable weights compared to a dense layer with the same number of hidden units. 
This reduces the propensity of the network to overfit to the training data and improves its ability to generalize to unseen examples.

Convolutional neural networks often incorporate \textit{pooling layers}, which reduce a small region of a grid (e.g. 2x2 pixels for an image) to a single grid element that takes either the maximum or average value (per channel) of the region. 
Pooling allows downsampling of the grid, and is often performed in between convolutional layers. 

The convolutions described above can only operate on a regular grid. 
Proteins, however, have inherently irregular structure. 
In this work we adapt the notion of convolution to work with irregular structures, which are represented as graphs. 





