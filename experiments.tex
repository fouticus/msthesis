\chapter{Experimental Setup}
\label{chap:experiments}

All of the spatial graph convolution methods described in Chapter \ref{chap:methods} were evaluated on the task of partner specific interface prediction and compared to the existing state of the art method.
This chapter outlines the training, validation, and testing data used for the evaluation, as well as the experimental procedure and relevant metrics.

\section{Data}

Each model was trained, validated, and tested using complexes from the Docking Benchmark Dataset (DBD) version 5.0~\cite{vreven2015}, a collection of 3D crystalline structures of transient complexes in both the bound and unbound formation. 
DBD is non-redundant in the sense that no two proteins in one complex are in the same Structural Classification of Proteins (SCOP)~\cite{murzin1995} families as two proteins in another complex, respectively.
This ensures the data set is appropriate for testing, since no complex in the test set will be too similar to a complex in the training set.
Each protein is classified into either \emph{rigid body}, \emph{medium difficulty}, or \emph{difficult}, based on the degree of conformational change upon forming the interface.
This is quantified using the root mean squared deviation of the alpha carbons after unbound and bound structures are superimposed on one another. 
Training and validation were performed using the 175 complexes that were present in version 4.0 of DBD, whereas the 55 new complexes added in DBD 5.0 were reserved for testing. 
Training and validation sets were created using a 80\%/20\% partition on the DBD v4.0 complexes, such that the difficulty class proportions in each partition were roughly equivalent.
There were 140 training complexes and 35 validation complexes.
Table \ref{tab:examples} indicates the number of positive and negative examples used from each dataset.

\begin{table}
	\centering
	\begin{tabular}{l l c r r}
		\toprule
		Phase & Data Set & Complexes & Positive examples  & Negative examples \\ 
		\midrule
		\multirow{2}{*}{Model Selection} 
			& Train      & 138       & 12,632 (9.1\%)     & 126,320 (90.9\%) \\
			& Validation & 34        & 3,042 (0.2\%) 		& 1,868,270 (99.8\%) \\
		\midrule
		\multirow{2}{*}{Testing}
			& Train + Validation & 175 & 16,004 (9.1\%) & 160,040 (90.9\%) \\
			& Test       & 55        & 4,871 (0.1\%)      & 4,953,446 (99.9\%) \\ 
		\bottomrule
	\end{tabular}
	\caption{Number of complexes and examples (residue pairs) for each phase of experimentation. Negative examples were down sampled to a ratio of 10:1 with positive examples when training, but testing included all examples. During model selection, three complexes were excluded from the training and validation sets due to a software bug and missing features. These complexes were eventually included for the testing phase. \label{tab:dataset_size}}
	\label{tab:examples}
\end{table}

3D structures are contained in Protein Data Bank (PDB) files, which contain atomic coordinates for each amino acid in the complexes.
True interfaces were determined using the bound formations, where two amino acids are assumed to be interacting if they are within 6 \AA{} of each other, consistent with prior work~\cite{ofran2007, ahmad2011, minhas2014}.
This definition permits a residue to interact with multiple partners in the interface.

\subsection{Vertex and Edge Features}
Prediction must be made using the unbound formation of each protein, since the bound formation is not known a priori.
Hence unbound PDB files were used to generate a graph representation of each protein, with vertices indicating residues and edges indicating the relationships between residues.
Features were then computed for each vertex and edge using sequence and structure information from the protein.
Vertex features pertain to the degree of residue conservation, accessible surface area, depth within the protein, and protrusion.
Edge features capture the distance between two residues and their relative orientation as well.
Raw distances are passed through a Gaussian function centered at zero with a standard deviation chosen during model selection.
More detail on each feature can be found in Appendix \ref{appendix:features}.

\subsection{Handling Missing Data}
\label{sect:missing_data}

In some cases, features could not be calculated for a residue, resulting in missing values.
For 13 residues, the atomic coordinates of the central alpha carbon were not present in the PDB files, so some features could not be calculated (see Appendix \ref{appendix:features} for more detail).
Most features rely on third party software to calculate the features from the PDB data.
For the proteins with DBD codes 2B42 and 3R9A, features related to the protrusion index could not be calculated for the receptor due to an unknown third party software fault.
These complexes were omitted during model selection but re-introduced with imputed values before testing.
Two other complexes, 1NW9 and 1PPE, were also excluded during model selection due to a software bug.
This bug was eventually fixed, so these were also included during testing. 
During model selection, missing values were imputed using the protein mean, whereas for testing, the global median (within the training, validation, or test set) was used.
In total there were 138 training and 34 validation complexes during model selection, and there were 175 training and 55 test complexes during testing.

\section{Experimental Procedure}

Experimentation was conducted in two phases: model selection, and final testing.
The model selection phase concerned the selection of model \emph{hyper-parameters}.
For this phase, pairwise graph convolutional networks were trained using the training set and evaluated on the validation set.
Once model selection was complete, the final set of hyper-parameters was used to train and test the most promising hyperparameter configurations and compare them.
For this phase, the model was trained on the combined training and validation sets and tested on the test set.
More detail about each phase follows.


\subsection{Model Selection}

Hyper-parameters comprise all model specifications besides the parameters themselves, such as the number and types of layers, number of units/filters in each layer, nonlinear activation function, loss function, and training procedure.
Hyper-parameters are typically chosen either by some automated process, or by manual exploration.

Methods for automated hyper-parameter selection include randomized search, grid search, or  sequential, model based optimization (SMBO) such as a Gaussian Process Optimization or Tree Structured Parzen Estimator Optimization~\cite{bergstra2011}.
The advantage of these approaches is their ability to automatically search a hyper-parameter space without human intervention. 
SMBO approaches are able to estimate the performance "response surface" in hyper-parameter space in order to efficiently find the optimum value.
The disadvantage of these approaches is they require a large number of iterations in order to converge to optimal parameters if the number of hyperparameters is large and the search space for each is large. 
Random or grid search are no better.
For this thesis, some experiments were performed with SMBO algorithms, but none produced hyper-parameters which outperformed results found by human exploration.
Therefore automated approaches were abandoned in favor of a manual search.

Several hyper-parameters were explored manually by training/validating networks under different settings of the hyper-parameters.
This exploration occurred in a one-hyper-parameter-at-a-time fashion, alleviating the search space problem encountered with the automated algorithms, but also limiting the ability to explore interactions between different hyper-parameters.
In some cases, the choice of hyperparameter affected training time and model convergence rate, but had little impact on overall model performance (e.g. the nonlinear activation function, training algorithm, the number of convolutional filters).
In other cases, few values had to be searched before a local optimum was found (e.g. the number of dense layers after merging, minibatch size, optimization strategy, residue distance Gaussian function standard deviation).
Because this manual optimization was iterative, it is impossible to summarize how each alernative  choice of hyperparameter would change the results from the set of hyperparameters ultimately chosen.
However, some trends were observed consistently throughout testing and are worth describing in more detail.
This is done below.

As the name implies, deep networks typically perform best with more than one layer. 
This is indeed the case for pairwise graph convolutional networks, although the benefit began to diminish beyond 4 layers.
To illustrate this trend, multiple depths were used during the testing phase.
Typically convolutional neural networks begin with a small receptive field size which grows in successive convolutional layers.
For this problem, networks performed better if all layers had the same relatively large receptive field, around 21. 
Significantly larger receptive fields would begin to encompass entire proteins, since the smallest protein in the data set has 29 residues.
During testing, receptive field sizes of 11 and 21 were used to enable comparison.
For image convolution, it is not uncommon for the number of filters in the first convolutional layer to be roughly an order of magnitude more than the number of input channels, and to increase in subsequent layers.
In contrast to images which may have only 1 or 3 channels, protein graphs have 70 features for each vertex, so an analogous increase in the number of filters would quickly exceed the memory resources of a GPU.
Nevertheless, a larger number of filters proved beneficial, so all graph convolutions started with 256 filters and increased to a size of 512 in the last convolutional layer (if the network had more than one).


Most model selection was performed on the novel graph convolutions and the "no convolution" case.
Because most of the the other convolution methods are not appreciably different, an abridged model selection process was performed for them, simply to check whether the same hyperparameters could reasonably be used.
The exceptional case was DCNN, where the Gaussian standard deviation for residue distances was observed to have a significant impact on model performance. 
For this method, rather than using a value of $\sigma = 18\AA{}$, much smaller values of $2\AA{}$ and $4\AA{}$ were chosen for the testing phase.
Given that the diffusion is occurring on a complete graph, the smaller values allow only local information to propagate during each hop.
The other graph convolutional methods do not have this same problem, since they are restricted to a fixed size receptive field. 
Diffusion convolutions do not downsample the input, so it is possible to stack multiple convolutions on top of one another.
However, additional diffusion convolutional layers did not improve performance, so only results from a single layer were tested.

\subsection{Model Testing Setup}

Final model testing was performed for all variants of convolution and the No Convolution case.
For each variant, receptive field sizes of 11 and 21 were tested (the central vertex plus 10 or 20 neighbors, respectively), with one, two, three, and four graph convolutional layers. 
In total, 16 combinations were tested, except DDCNN as already noted.
All variants followed the same training scheme:
The number of filters for networks with one convolutional layer was 256, Likewise for two, three, and four convolutional layers the number of filters was (256, 512), (256, 256, 512), and (256, 256, 512, 512) respectively.
For DTNN, the number of filters necessarily matched the number of input features, 70.
After merging examples via concatenation, dense layers of 256 and 1 units were applied, with the latter producing the model output.
Except for the last layer, rectified linear units (ReLU) as the nonlinear activation function, except for DTNN for reasons explained below.
Note that activation functions are being applied to each neighbor individually in Equation (\ref{eq:deep_tensor}), which is different from the other methods.
Since ReLU is non-negative, it would force the layer output to be greater or equal to the input.
Therefore, tanh is used instead to allow for both positive and negative adjustments.

Loss was computed using weighted cross entropy, a measure of the dissimilarity between two probability distributions.
Since this is a binary classification problem, the cross entropy for a residue pair $x_i$ is defined as: 

\begin{equation}
\mathbb{H}(x_i, y_i | \Theta) = - r ~ y_{i0} ~ log\big(f_0(x_i|\Theta)\big) - y_{i1} ~ log\big(f_1(x_i|\Theta)\big),
\label{eq:weighted_ce}
\end{equation}

\noindent
where $f = (f_0, f_1)$ is the softmax network output, interpreted as class probabilities, $y_i = (y_{i0}, y_{i1})$ is the one-hot class label, and $r$ is the ratio of positive to negative examples, which ensures that the set of positive examples and the set of negative examples contribute equally to the loss function.
The overall loss function is then:

\begin{equation}
\mathbb{L}(\Theta | \{x_i, y_i\}) = \sum_{i} \mathbb{H}(x_i, y_i | \Theta),
\end{equation}

\noindent
which sums the cross entropy from each residue pair.


All bias weights were initialized to zero.
All non-bias weights were initialized by drawing from a uniform distribution between $-\tau_0$ and $\tau_0$, where $\tau_0=\frac{1}{\sqrt{n_{in}}}$ and $n_{in}$ is the number of inputs to the layer.
This is similar to the initialization proposed by He, Zhang, Ren \& Sun~\cite{he2015}, except they differ by the small multiplicative constant, $\sqrt{6}$.
Stochastic gradient descent was performed with a learning rate of 0.1, mini-batch size of 128 pairs, for 80 epochs.
Dropout with probability $p=0.5$ was performed during training.

The networks were implemented in TensorFlow~\cite{abadi2015} v1.0.1 and trained using a single NVIDIA GTX 980 or GTX TITAN X GPU.
Training time varied from roughly 7 to 220 minutes depending on convolution method, implementation, network size, GPU card, and computer resource availability.
Most methods took less than 60 minutes, with the exceptions being 4 layer PATCHY-SAN networks and R-GCNs that include basis functions.


The graph convolution variants were compared to PAIRpred, an existing state-of-the-art partner specific interface prediction algorithm based on an SVM with custom symmetric pairwise kernels, proposed by Minhas et al.~\cite{minhas2014}.
PAIRpred was run with the same features and data that were used in this work for the purpose of comparison.
Grid search with five-fold cross validation was used to select the optimal kernel and soft margin parameters.



\section{Metrics}

While measures like accuracy, precision, recall, specificity, and F-score are common for many classification problems, they are dependent on the choice of a particular classification threshold.
A metric more appropriate for interface prediction would be the area under the receiver operating characteristic curve (AUC), because it does not rely on a particular choice of threshold.
Instead, it summarizes the true positive rate across all false positive rates, indicating threshold independent performance of the classifier.

In addition, in the case of an extreme class imbalance, the AUC is misleading, since the movement of a single positive example can heavily influence the true positive rate compared to the effect that a single negative example has on the false positive rate.
Even with a high AUC, a classifier may rank a number of negative examples above the first positive examples, when ranking by predicted likelihood of being part of the interface.
In this case, it is also useful to measure how far down the ranked scores one has to go before encountering a true positive.
A metric which captures this directly is the rank of the first positive prediction (RFPP), where the rank is computed after ordering pairs by classifier score.
Ideally, the top ranking prediction is truly part of the interface, in which case RFPP is 1.

Interface prediction occurs at the complex level, so AUC and RFPP should be calculated with respect to each complex. 
The performance of a classifier on a set of complexes can then be summarized using median AUC and RFPP across the complexes.
These metrics closely mimic those used in the PAIRpred paper~\cite{minhas2014}, except that median AUC is taken instead of average.
