\chapter{Methods}
\label{chap:methods}

The methods presented in this thesis were drawn from a desire to exploit local structure around a residue when performing interface prediction.
The biological motivation for this is that a residue's neighborhood influences its propensity to participate in an interface.
It was noted in Chapter \ref{chap:neuralnetworks} that convolutional neural networks are one way of detecting features in a local neighborhood, but they are limited to regular grids. 
Unfortunately, proteins cannot naturally be represented as a regular grid, so convolution must be developed for a more natural representation: graphs.


\section{Proteins As Graphs}

An undirected, unweighted graph $G$ consists of a set of vertices, $V=\{v_1, v_2, ..., v_n\}$, and a set of eges, $E=\{e_1, e_2, ..., e_m\}$ where each edge is incident to two vertices and there is at most one edge between two vertices.
One way of representing proteins as graphs is to let each vertex represent a residue in the protein, and each edge represent the relationship between two residues.
Thus any information pertaining to a particular residue can be associated with the relevant vertex in the form of a feature vector.
The features used in this work are drawn from features used in prior interface prediction work \cite{minhas2014}.
Likewise, any information about the relationship between two residues can be associated with the relevant edge.
The edge features used here describe the distance between and relative orientation of two residues.
These edge features are defined between any two residues in the protein, so the graph is complete. 
The number of edge features and number of vertex features may not be the same.
A detailed explanation of each feature is contained in Appendix \ref{appendix:features}

This representation is a structural abstraction of the original protein to a well studied mathematical object.
It does not rely on a coordinate system, as is the case when working with raw 3D positions.
This makes biological sense because proteins often have no natural orientation in the cell, and the relative orientations of two interacting proteins is not known a priori.
However, the graph retains 3D structural information in the edge features, and is embedded in an underlying metric space.
This embedding is useful when defining local neighborhoods around vertices, which is necessary when designing a convolution.
With proteins represented as graphs, the remaining task is to design a convolution operation which operates on graphs. 

\section{Graph Convolution}
Though the formulation and application of graph convolution presented in this thesis are new, graph convolution exists in the literature.


\subsection{Prior Work in Graph Convolution}
Recent years have seen increased attention for problems involving graph structured data, prompting developments in graph convolution to perform various tasks on those data~\cite{bronstein2016}.
This allows leveraging of deep learning techniques, which have seen great success for problems on regular grids.
Graph convolution approaches have generally fallen into two categories, \textit{spectral} and \textit{spatial}.

Spectral methods are based on linear functions in the "frequency domain" of a graph, defined using the laplacian operator $\mathcal{L}=I-D^{-1/2}WD^{-1/2}$, where $I$ is the identity matrix, $W$ is a similarity matrix (containing edge weights), and $D$ is a diagonal matrix containing the degree of each vertex~\cite{bruna2013, henaff2015, kipf2016}.
%TODO: mention(?): scaling to large graphs difficult because factoring large matrices difficult
Each filter in a spectral convolution implies a weighting of each frequency in the spectral decomposition of the graph~\cite{mallat2009}.

Spatial methods instead define operations in a localized neighborhood of a central vertex~\cite{henaff2015, atwood2016diffusion}.
Each neighborhood constitutes a receptive field where a convolution operation is performed. 
Convolution commonly involves a vector of weights and take a weighted sum of neighbors, much like a discrete convolution on a grid can be viewed as taking a weighted sum of grid elements within the receptive field.
%TODO: cite papers with different neighborhood convolution (write more about them?)
Spatial convolution is more directly analogous to grid based convolution as described in Chapter \ref{chap:neuralnetworks}, but introduce a problem of correspondence when translated to graphs.

\subsection{The Problem of Receptive Field Correspondence}
When convolving on a grid, each receptive field has an identical structure (for example 3x3 pixels in an image), so there is an automatic correspondence between receptive fields, such  that the same weights are applied to corresponding portions of all receptive fields. 
For example, the upper left pixel in a 3x3 receptive field is always multiplied by the same weight when taking the weighted sum, regardless of which receptive field is being considered.
With graphs, there is often no such correspondence from one receptive field to another (there is no "upper left" vertex in a vertex neighborhood), aside from the natural correspondence of central residues. Hence the issue is what to do with the neighbors. To complicate things, the number of neighbors in a receptive field may not be constant and is dependent on how the receptive field is defined.
This problem of correspondence has traditionally been addressed with two approaches that are summarized below. 
\begin{enumerate}
	\item \textit{Imposed ordering of neighbors}. This approach generates a correspondence between two receptive fields by ordering the neighbors in each and associating neighbors of a common position. 
	Ordering methods can be based on vertex characteristics, like degree and betweenness centrality, or some domain specific knowledge ~\cite{niepert2016, duvenaud2015}.
	They also typically require the number of neighbors in a receptive field to remain the same.
	This approach allows filter weights which are tied to a particular position in the ordering, which, it is assumed, has some invariant significance across all receptive fields.
	However, the imposed ordering is often arbitrary, limiting the usefulness of this approach.
	
	\item \textit{Identical treatment of neighbors}. This approach ignores the need to establish a correspondence between receptive fields and instead treats all neighbors identically.
	Rather than applying different weights to neighbors depending on their position in an ordering, the same weights are applied to each neighbor.
	This allows for different sizes of receptive fields and avoids choosing an ordering method, but lacks the ability to treat neighbors differently based on their relationships to each other and to the central vertex.
	However, this method fails to capture the possible unique structural relationship between each neighbor and the central vertex.
\end{enumerate}
Figure \ref{fig:correspondence_approaches} shows a graphical depiction of both approaches.

\begin{figure}
	\centering
	%\begin{center}
	\includegraphics[width=0.8\textwidth]{correspondence_approaches.pdf}
	%\end{center}
	\caption{Two approaches of establishing correspondence between the neighbors of receptive fields A and B. Central vertices are shown in blue and neighbors in green. The central vertices always correspond with one another. Left: neighbors are ordered and associated based on position. Unique weights (\textit{$w_2$--$w_4$}) can then be applied to each position in the order. Right: neighbors are left unordered and treated identically. This requires that the same weights (\textit{$w_2$}) be used for all neighbors.}
	\label{fig:correspondence_approaches}
\end{figure}


\subsection{Order-Free Coupled Graph Convolution}
This thesis presents a graph convolution which avoids imposing an arbitrary ordering on the neighbors in a receptive field but also avoids treating all neighbors the same.
This is accomplished by incorporating information from the edges between each neighbor and the central vertex.
Here are two variants of graph convolution which differ in how the edge information is incorporated, denoted \textit{sum coupled} and \textit{product coupled}.
For a central vertex $i$ on the graph and a local neighborhood of vertices $\mathcal{N}_i$, the output of \emph{sum coupled} graph convolution is:
\begin{equation}
h_i(x | W^\textsc{c}, W^\textsc{n}, W^\textsc{e}, b) = \sigma \bigg( W^{\textsc{c}} x_i + \frac{1}{|\mathcal{N}_i|}\sum_{j \in \mathcal{N}_i} (W^{\textsc{n}} x_j + W^{\textsc{e}} A_{ij}) + b \bigg),
\label{eq:sum_coupling}
\end{equation}
where $x_i$ is the feature vector associated with vertex $i$, $A_{ij}$ is the feature vector associated with edge $(i, j)$, $W^\textsc{c}$, $W^\textsc{n}$ and $W^\textsc{e}$ are weight matrices, and $b$ is a vector of biases. 
If there are $l$ vertex channels, $p$ edge channels, and $k$ filters, then $W^\textsc{c}\in\mathbb{R}^{k \times l}$, $W^\textsc{n}\in\mathbb{R}^{k \times l}$, $W^\textsc{e}\in\mathbb{R}^{k \times p}$, and $b\in\mathbb{R}^{k}$.
Intuitively, this calculates an activation for the central vertex, each neighbor vertex, and each edge between a neighbor and the central vertex separately.
It is the inclusion of edge activations that allows each neighbor to be distinguished from the others on the basis of its relationship to the central vertex.
This variant is called sum coupled because the neighbor vertex and edge activations are added together.
Because of this, the direct association between a neighbor its edge is lost.
A variant which maintains the association is \emph{product coupling}, whose output is:
\begin{equation}
h_i(x | W^\textsc{c}, W^\textsc{n}, W^\textsc{e}, b) = \sigma \bigg( W^{\textsc{c}} x_i + \frac{1}{|\mathcal{N}_i|}\sum_{j \in \mathcal{N}_i} (W^{\textsc{n}} x_j \odot W^{\textsc{e}} A_{ij}) + b \bigg),
\label{eq:prod_coupling}
\end{equation}
where $\odot$ denotes the elementwise product between two vectors or matrices. 
This allows a neighbor's influence on the overall activation to be modulated by its relationship to the central vertex.
For protein graphs, this means neighboring residues will contribute more or less to the overall activation, depending on their distance from and relative orientation to the central vertex, with the precise modulation determined by the edge activations.

Just as convolution on a regular grid can be applied at any pixel in an image, graph convolution can be applied to any vertex in the graph.
Hence both can be considered translation invariant.

Both sum coupled and product coupled graph convolution are translation invariant, don't impose an ordering in the neighbors or a correspondence between receptive fields of any kind, allow for different numbers of neighbors, and account for the different relationships between neighbors and the central vertex. 
The receptive fields are always defined around a central vertex, so the results of convolution can be applied to that vertex.
This retains the graph structure after each convolution, so convolutional layers are stackable, as with images.

A note on receptive fields: protein graphs are complete and embedded in a metric space, so this thesis defines a receptive field using a fixed number of closest neighbors to the central vertex.
A receptive field can also be defined using a threshold $\delta>0$ such that all vertices closer to the central vertex than the threshold are included in the receptive field.
All neighbors in a receptive field are guaranteed to share an edge with the central vertex, allowing the application of equations (\ref{eq:sum_coupling}) and (\ref{eq:prod_coupling}).
For incomplete graphs, a receptive field can be defined as all vertices within $k$ hops of the central vertex. 
If $k=1$, both versions of graph convolution can directly be applied.
If $k>1$, then product coupled graph convolution can't be directly applied to neighbors more than 1 hop away from the center vertex, since they share no edge with the center. 
Though there are ways to adapt product coupled graph convolution in this situation, they are not the focus of this thesis.
%TODO: say more?

Lastly, to assess the benefit that incorporating information from neighboring residues has on classification performance, the \emph{no convolution} variant is defined as:

\begin{equation}
h_i(x | W^\textsc{c}, b)= \sigma \bigg( W^{\textsc{c}} x_i + b \bigg),
\label{eq:no_conv}
\end{equation}

which resembles the activation of a dense layer in regular neural networks.


\section{Pairwise Neural Network Architecture}
These graph convolution operations allow the detection of local patterns on a single graph, and produce a new representation at each vertex.
Partner specific protein interaction, however, requires classifying pairs of residues in different proteins (vertices in different graphs), which is essentially making predictions on vertices in the product graph. 
Such predictions are made using a pairwise neural network architecture.

A pairwise architecture consists first of two identical convolutional modules, each responsible for generating the representation for one of the proteins in the pair.
These are the \emph{pre-merge} layers.
A key requirement for the pairwise architecture is symmetry, since the prediction for a pair of residues should be the same irrespective of which leg is used for each protein.
To ensure symmetry in the pre-merge layers, weights are shared between layers in the different legs.
The merge layer then combines the vertex representations from one graph with the vertex representations from the other into pairs.
To maintain symmetry, This merge process should also be symmetric.
For example, the elementwise sum, elementwise product, and outer-product are all commutative and therefore produce symmetric output.
Another option is to combine pairs asymmetrically, such as concatenating the two representations together, but then average the predictions from each ordering of the representations.
%TODO: talk about pairwise kernels?
Finally, the combined representation for each pair of residues is passed through a number of post-merge layers.
The data are represented as pairs of residues at this point. 
Theoretically, graph convolution could be performed at this stage as well, this time in the product graph.
However, the computational and memory requirements of doing so prove prohibitive since the number of convolutions and the number of neighbors in each convolution increases quadratically.
Hence the work in this thesis performs no convolution after merging.
The final layer has a single output for each pair indicating the prediction for that pair.
This output is be compared to an encoded label indicating whether or not the pair constitute part of the true interface. 
See Figure \ref{fig:pairwise_arch1} for a graphical depiction.

\begin{figure}
	\includegraphics{pairwise_network2.pdf}
	\caption{A pairwise neural network architecture that takes two protein graphs as input. Each leg contains one or more convolutional layers. The resultant graphs are then merged to create representations of residue pairs. After more post-merge layers, a final classification is performed for each pair.}
	\label{fig:pairwise_arch1}
\end{figure}


This chapter has presented protein graphs, graph convolution operations, and pairwise neural network architectures, all of which are components in this deep learning approach partner specific protein interface prediction.
Chapter \ref{chap:experiments} describes the experiments that were performed and gives a discussion of the results.
